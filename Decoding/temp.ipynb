{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from common import constants\n",
    "from common.mwoz_data import CustomMwozDataset\n",
    "from common.utils import compute_prediction_scores\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_type = 'mistral' if 'mistral' in model_name.lower() else 'gemma'\n",
    "        self.response_marker = \"[/INST]\" if self.model_type == 'mistral' else \"ASSISTANT:\"\n",
    "        self.model, self.tokenizer = self._load_model()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        # Load in the same way as training\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        \n",
    "        # Model specific configurations\n",
    "        model_kwargs = {\n",
    "            \"quantization_config\": bnb_config,\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "        }\n",
    "        \n",
    "        if self.model_type == 'mistral':\n",
    "            model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "        else:\n",
    "            model_kwargs[\"attn_implementation\"] = \"eager\"\n",
    "            \n",
    "        print(f\"Loading model from {constants.MERGED_MODEL[self.model_name]}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            constants.MERGED_MODEL[self.model_name],\n",
    "            **model_kwargs\n",
    "        )\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(constants.MERGED_MODEL[self.model_name])\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = 'right'\n",
    "        \n",
    "        return model, tokenizer\n",
    "    \n",
    "    def decode_response(self, output_ids: torch.Tensor, to_print: bool = False) -> str:\n",
    "        \"\"\"Decode model output following training format\"\"\"\n",
    "        decoded = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        \n",
    "        if to_print:\n",
    "            print(\"Raw response:\", decoded)\n",
    "            \n",
    "        # Extract response part\n",
    "        if self.response_marker in decoded:\n",
    "            et_preds = decoded.split(self.response_marker)[1].strip()\n",
    "        else:\n",
    "            et_preds = decoded\n",
    "            \n",
    "        # Handle proper formats as in training\n",
    "        if 'no entity' in et_preds:\n",
    "            et_preds = '[no entity]'\n",
    "        else:\n",
    "            et_preds = et_preds.replace('[', '').replace(']', '').strip()\n",
    "            \n",
    "        if to_print:\n",
    "            print(\"Cleaned response:\", et_preds)\n",
    "            \n",
    "        return et_preds\n",
    "    \n",
    "    def generate_with_strategy(self, \n",
    "                             input_ids: torch.Tensor,\n",
    "                             attention_mask: torch.Tensor,\n",
    "                             strategy: str = \"greedy\",\n",
    "                             **kwargs) -> List[str]:\n",
    "        \"\"\"Generate responses with different decoding strategies\"\"\"\n",
    "        \n",
    "        generation_config = {\n",
    "            \"max_new_tokens\": constants.MAX_NEW_TOKENS,\n",
    "            \"do_sample\": False,\n",
    "            \"num_beams\": 1,\n",
    "        }\n",
    "        \n",
    "        if strategy == \"beam\":\n",
    "            generation_config.update({\n",
    "                \"num_beams\": kwargs.get(\"num_beams\", 5),\n",
    "                \"length_penalty\": kwargs.get(\"length_penalty\", 0.6),\n",
    "                \"num_return_sequences\": kwargs.get(\"num_return_sequences\", 1),\n",
    "                \"no_repeat_ngram_size\": 3,\n",
    "            })\n",
    "        elif strategy == \"sampling\":\n",
    "            generation_config.update({\n",
    "                \"do_sample\": True,\n",
    "                \"num_return_sequences\": kwargs.get(\"num_samples\", 5),\n",
    "                \"temperature\": kwargs.get(\"temperature\", 0.7),\n",
    "                \"top_p\": kwargs.get(\"top_p\", 0.9),\n",
    "            })\n",
    "            \n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **generation_config\n",
    "        )\n",
    "        \n",
    "        # Handle multiple sequences\n",
    "        if (strategy == \"beam\" and kwargs.get(\"num_return_sequences\", 1) > 1) or \\\n",
    "           (strategy == \"sampling\" and kwargs.get(\"num_samples\", 1) > 1):\n",
    "            outputs = outputs.reshape(input_ids.shape[0], -1, outputs.shape[-1])\n",
    "            responses = []\n",
    "            for batch_outputs in outputs:\n",
    "                batch_responses = []\n",
    "                for sequence in batch_outputs:\n",
    "                    generated_ids = sequence[input_ids.shape[1]:]\n",
    "                    response = self.decode_response(generated_ids)\n",
    "                    batch_responses.append(response)\n",
    "                \n",
    "                if strategy == \"sampling\":\n",
    "                    # For sampling, use majority voting\n",
    "                    from collections import Counter\n",
    "                    response = Counter(batch_responses).most_common(1)[0][0]\n",
    "                else:\n",
    "                    # For beam search, use first (highest probability) response\n",
    "                    response = batch_responses[0]\n",
    "                    \n",
    "                responses.append(response)\n",
    "            return responses\n",
    "        else:\n",
    "            generated_ids = outputs[:, input_ids.shape[1]:]\n",
    "            responses = [self.decode_response(ids) for ids in generated_ids]\n",
    "            return responses\n",
    "    \n",
    "    def evaluate(self, \n",
    "                test_data_path: str,\n",
    "                strategy: str = \"greedy\",\n",
    "                batch_size: int = 1,\n",
    "                save_results: bool = True,\n",
    "                **generation_kwargs) -> Dict:\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        \n",
    "        print(f\"\\nEvaluating {self.model_name} using {strategy} decoding...\")\n",
    "        \n",
    "        # Load test dataset\n",
    "        test_dataset = CustomMwozDataset(\n",
    "            self.tokenizer,\n",
    "            data_filename=test_data_path,\n",
    "            model_type=self.model_type,\n",
    "            mode='infer'\n",
    "        ).data\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        all_responses = []\n",
    "        all_ground_truths = []\n",
    "        \n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Generating responses\"):\n",
    "                # Prepare inputs\n",
    "                formatted_prompts = batch['text']\n",
    "                inputs = self.tokenizer(\n",
    "                    formatted_prompts,\n",
    "                    padding=\"longest\",\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Generate responses\n",
    "                responses = self.generate_with_strategy(\n",
    "                    inputs['input_ids'],\n",
    "                    inputs['attention_mask'],\n",
    "                    strategy=strategy,\n",
    "                    **generation_kwargs\n",
    "                )\n",
    "                \n",
    "                all_responses.extend(responses)\n",
    "                all_ground_truths = test_dataset\n",
    "        \n",
    "        # Compute metrics\n",
    "        prec, rec, f1, accuracy = compute_prediction_scores(all_responses, all_ground_truths)\n",
    "        \n",
    "        metrics = {\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1\": f1,\n",
    "            \"accuracy\": accuracy\n",
    "        }\n",
    "        \n",
    "        print(f\"Results for {self.model_name} using {strategy} decoding:\")\n",
    "        print(json.dumps(metrics, indent=2))\n",
    "        \n",
    "        if save_results:\n",
    "            output = []\n",
    "            for idx, resp in enumerate(all_responses):\n",
    "                tetypes = resp.split('|')\n",
    "                tetypes = [x.strip() for x in tetypes if '[no entity]' != x]\n",
    "                output.append({\n",
    "                    'uuid': test_dataset[idx]['uuid'],\n",
    "                    'turn_id': test_dataset[idx]['turn_id'],\n",
    "                    'prediction': tetypes\n",
    "                })\n",
    "                \n",
    "            save_path = f\"{constants.TEST_RESULT_FILE[self.model_name]}.{strategy}\"\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(output, f, indent=2)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(constants.SEED)\n",
    "    torch.manual_seed(constants.SEED)\n",
    "    \n",
    "    # Models to evaluate\n",
    "    models = ['mistral-entity-pred', 'gemma-entity-pred']\n",
    "    \n",
    "    # Decoding strategies and their parameters\n",
    "    strategies = {\n",
    "        \"greedy\": {},\n",
    "        \"beam\": {\n",
    "            \"num_beams\": 5,\n",
    "            \"length_penalty\": 0.6\n",
    "        },\n",
    "        \"sampling\": {\n",
    "            \"num_samples\": 5,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Store all results\n",
    "    all_results = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        evaluator = ModelEvaluator(model_name)\n",
    "        model_results = {}\n",
    "        \n",
    "        for strategy, params in strategies.items():\n",
    "            results = evaluator.evaluate(\n",
    "                test_data_path=f'{constants.DATA_DIR}test.json',\n",
    "                strategy=strategy,\n",
    "                batch_size=1,  # Can be increased based on GPU memory\n",
    "                **params\n",
    "            )\n",
    "            model_results[strategy] = results\n",
    "            \n",
    "        all_results[model_name] = model_results\n",
    "    \n",
    "    # Save overall results\n",
    "    print(\"\\nAll Results:\")\n",
    "    print(json.dumps(all_results, indent=2))\n",
    "    \n",
    "    with open('evaluation_results.json', 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
